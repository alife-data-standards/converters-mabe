import json
import csv
import os
import argparse

VALID_STD_OUTPUT_TYPES=["csv", "json"]
VALID_MABE_OUTPUT_TYPES=["snapshot", "sswd"]
MABE_ANCESTOR_LIST_COLUMN_NAME = {
    "sswd": "ancestors_LIST",
    "snapshot": "snapshotAncestors_LIST"
}

def ConvertMABESnapshotsToStdPhylogeny(mabe_output_path, mabe_output_type="snapshot", std_output_file_path="lineage.csv", std_output_type="csv", column_name_transforms={}, columns_of_interest=None, verbose=False):
    """
    """
    mabe_output_type = mabe_output_type.lower()
    std_output_type = std_output_type.lower()

    # Validate arguments
    # - path
    if not os.path.isdir(mabe_output_path):
        raise Exception(f"Invalid path: {mabe_output_path}")
    # - file type
    if mabe_output_type not in VALID_MABE_OUTPUT_TYPES:
        raise Exception(f"Failed to recognize fileType ({mabe_output_type}). Valid files types are: {VALID_MABE_OUTPUT_TYPES}")
    # - std output type
    if std_output_type not in VALID_STD_OUTPUT_TYPES:
        raise Exception(f"Invalid standard output type ({std_output_type}). Valid types include: {VALID_STD_OUTPUT_TYPES}")

    ancestorsColumnName = MABE_ANCESTOR_LIST_COLUMN_NAME[mabe_output_type]

    # These columns are always of interest
    ofInterest = set(["ID", "timeOfBirth", ancestorsColumnName])

    name_map = column_name_transforms
    name_map["ID"] = "id"
    name_map[ancestorsColumnName] = "ancestor_list"
    name_map["timeOfBirth"] = "origin_time"

    # -- bookmark --
    # Get a list of files
    files = [f for f in os.listdir(mabe_output_path) if ".csv" in f and "_data_" in f]
    files = sorted(files, key=lambda f: int(f.split(".")[0].split("_")[-1])) # Sort into correct order.

    rawData = {}
    header = []
    header_lu = {}

    mustGetHeader = True
    for fname in files:
        fpath = os.path.join(mabe_output_path, fname)
        with open(fpath, 'r') as csvfile:
            if(verbose):
                print('loading', fname)
            data = csv.reader(csvfile, delimiter=',', quotechar='"')
            firstLineInFile = True
            for line in data:
                if firstLineInFile: # first line of file, do not recored to raw data
                    firstLineInFile = False
                    if mustGetHeader: # first line, of first file; keep it as header
                        mustGetHeader = False
                        header = line
                        header_lu = {value:index for index, value in enumerate(line)}
                else:
                    rawData[line[header_lu["ID"]]] = line # if same ID appears more then once, save the last appearance

    if columns_of_interest is None:
        # Otherwise, untion current of interest with what we found
        ofInterest = ofInterest.union(set(list(header)))
    else:
        # If provided with specifics on columns of interest, add them here.
        ofInterest = ofInterest.union(set(list(columns_of_interest)))

    # Verify that everything in ofInterest is actually in the header
    for item in ofInterest:
        if item not in header: raise Exception(f"Failed to find column {item} in MABE output file")

    #construct a data object we can serialize
    outData = {}

    for orgID in rawData:
        dataOnThisLine = {}

        for thing in header:
            if thing not in ofInterest:
                continue
            old_col_name = thing
            new_col_name =  name_map[old_col_name] if old_col_name in name_map else old_col_name

            col_index = header_lu[old_col_name]
            if old_col_name[-5:] == "_LIST":
                if (rawData[orgID][col_index][-1] == ','):
                    rawData[orgID][col_index] = rawData[orgID][col_index][:-1]
                dataOnThisLine[new_col_name] = [float(s) for s in rawData[orgID][col_index].split(",")] # NOTE: This line will break if lists every have non-numeric data in them!
            else:
                dataOnThisLine[new_col_name] = rawData[orgID][col_index]
        outData[orgID] = dataOnThisLine

    new_header = [name_map[h] if h in name_map else h for h in header if h in ofInterest]

    if std_output_type == "csv":
        with open(std_output_file_path, 'w') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=new_header, lineterminator="\n")
            writer.writeheader()
            for orgID in rawData:
                writer.writerow(outData[orgID])

    if std_output_type == "json":
        with open(std_output_file_path, 'w') as fp:
            json.dump(outData, fp, sort_keys=True, indent=4)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Converts .csv files generated by MABE to the standard phylogeny format')
    parser.add_argument('-path', type=str, metavar='PATH', default = '.', help='path to files - default : none (will read files in current directory)', required=False)
    parser.add_argument('-fileType', type=str, metavar='TYPE', default = 'snapshot', help='type of file, either snapshot or SSwD; default: snapshot', required=False)
    parser.add_argument("-output_file_path", "-out", type=str, default="lineage.csv", help="Name to assign to standard-compliant output file.")
    parser.add_argument("-format", "-f", type=str, default="csv", help="What standard file format should this script output? Valid options: {}".format(VALID_STD_OUTPUT_TYPES))
    parser.add_argument('-verbose', action='store_true', default = False, help='adding -verbose will provide more text output while running (useful if you are working with a lot of data to make sure that you are not hanging) - default (if not set) : OFF', required=False)

    parser.add_argument('-oldColumnNames', type=str, metavar='COLUMN_NAME', default = None,  help='column names of data to read from source files - default : "score_AVE" ("ID", "timeOfBirth", and the correct ancestors list are added to the list automatically)',nargs='+', required=False)
    parser.add_argument('-newColumnNames', type=str, metavar='COLUMN_NAME', default = [],  help='column names of data to be copied into new data file - default : NONE, if blank, copy oldColumnNames ("id", "origin_time", and "parents" are added to the list automatically)',nargs='+', required=False)

    args = parser.parse_args()

    # Process old column names and new column names
    cols_of_interest = args.oldColumnNames
    col_new_names = args.newColumnNames
    col_name_map = {}
    if not cols_of_interest is None and len(col_new_names) > 0:
        if len(cols_of_interest) != len(col_new_names):
            raise Exception("To remap oldColumnNames, you must provide remapping for all columns with newColumnNames")
        col_name_map = {value:col_new_names[index] for index, value in enumerate(cols_of_interest)}
        print(f"REMAPPING: {col_name_map}")
    ConvertMABESnapshotsToStdPhylogeny(mabe_output_path=args.path, mabe_output_type=args.fileType, std_output_type=args.format, std_output_file_path=args.output_file_path, columns_of_interest=cols_of_interest, column_name_transforms=col_name_map)